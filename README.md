# Data Warehouse with S3 and Redshift
This project has as output a Data Warehouse solution. 
It includes building an ETL pipeline that extracts their data from [AWS S3](https://aws.amazon.com/s3/), 
stages them in [Amazon Redshift](https://aws.amazon.com/es/redshift/), 
and transforms data into a set of dimensional tables for their analytics team 
to continue finding insights in what songs their users are listening to.
 
 Sparkify is a fictional popular digital media service created by Udacity, similar to Spotify or 
 Pandora; many users use their services every day.
 
 ### Prerequisites
The environment needed for this project:
1. [Python 3.6](https://www.python.org/downloads/release/python-360/)
2. [configparser: Configuration file parser](https://docs.python.org/3/library/configparser.html)
3. [Psycopg â€“ PostgreSQL database adapter for Python](https://www.psycopg.org/docs/)
4. [sql_query ](https://pypi.org/project/sql-queries/)

### Explanation of the files in the repository
1. **create_tables.py:** python code to drop (if exist) and  create fact and dimension tables for the star schema in Redshift.
2. **etl.py:** python code to load data from S3 (json files) into staging tables on Redshift and then process that data into your analytics tables on Redshift.
3. **sql_queries.py:** SQL statements, which will be imported into the two other files above. It has `DROP TABLE`, `CREATE TABLE`, `COPY` and `INSERT` statements.
4. **dwh.cfg:** configuration file that contains data connections to: 
    - Redshit cluster
    - IAM_ROLE
    - AWS S3  
    Note: check the `Data Warehouse configurations and setup` section and read the file to replace with your own redshift 
    cluster and IAM role. 
5. **images:** folder that contains the images used in this file.

### Data Warehouse configurations and setup
1. Create an IAM Role
    - Go to [Amazon IAM console](https://console.aws.amazon.com/iam)
    - Choose Create role.
    - In the AWS Service group, choose `Redshift`.
    - Under Select your use case, choose `Redshift - Customizable`, and then Next: Permissions.
    - On the Attach permissions policies page, choose `AmazonS3ReadOnlyAccess`
2. Create Security Group
    - Go to [Amazon EC2 console ](https://console.aws.amazon.com/ec2) and under Network and 
    Security in the left navigation pane, select Security Groups.
    - Click on `Create Security Group`
    - Enter a Security group name and description.
    - Select the `Inbound` tab under `Security group rules.`
    - Click on Add Rule and enter the following values:
        - Type: Custom TCP Rule.
        - Protocol: TCP.
        - Port Range: 5439. The default port for Amazon Redshift is 5439, but your port might be different. See note on determining your firewall rules on the earlier "AWS Setup Instructions" page in this lesson.
        - Source: select Custom IP, then type 0.0.0.0/0. Note: or select a specific location to share it.
3. Launch a Redshift Cluster
    - Sign in to the AWS Management Console and open the [Amazon Redshift console](https://console.aws.amazon.com/redshift/)
    - On the Amazon Redshift Dashboard, choose `Launch cluster`.
    - On the Cluster details page, configure `Cluster identifier`, `Database name`, 
    `Database port`, `Master user name` and `Master user password` 
    - select Node time
    - On the Additional Configuration page, enter the following values:
        - Available IAM roles: IAM Role just created
        - VPC security groups: security group created previously
    - choose Launch cluster.

### Instructions to run the project
1. clone the github repository: `git clone https://github.com/Erickramirez/Sparkify_Data_Warehouse.git`
2. verify the Prerequisites
3. Create the tables using the command `python create_tables.py`
4. Execute ETL process using the command `python etl.py`

## About the Data Warehouse solution
### Datasets
1. **Song data:** `s3://udacity-dend/song_data`  it is a subset of real data from 
[Million Song Dataset](http://millionsongdataset.com/) it is in JSON format: 
    ```
    {
        "num_songs": 1,
        "artist_id": "ARJIE2Y1187B994AB7",
        "artist_latitude": null,
        "artist_longitude": null,
        "artist_location": "",
        "artist_name": "Line Renaud",
        "song_id": "SOUPIRU12A6D4FA1E1",
        "title": "Der Kleine Dompfaff",
        "duration": 152.92036,
        "year": 0
    }
    ```
2. **Log data:** `s3://udacity-dend/log_data` consists of log files in JSON format 
generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. 
It has the following structure:  ![log-data](/images/log-data.png)
### Database Schema
It will be necesary to copy the JSON file in the S3 buckets into the staging tables. 
After this staging tables, the data is loaded into dimension and fact tables. 
This table definition is in `sql_queries.py`
1. **Staging tables**
    - **staging_songs:**  contains the data from the dataset `Song data` 
    - **staging_events:**  contains the data from the dataset `Log data` 
2. **Dimension tables**
    - **users:** users in the app - user_id, first_name, last_name, gender, level
    - **songs:** songs in music database - song_id, title, artist_id, year, duration
    - **artists:** artists in music database - artist_id, name, location, lattitude, longitude
    - **time:** timestamps of records in songplays broken down into specific units - start_time, hour, day, week, month, year, weekday
3. **Fact table**
    - **songplays:** records in event data associated with song plays i.e. records with page NextSong - 
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
    
### ETL Pipeline
- Create tables, this is in `Database Schema` section
- Load the data from S3 buckets into staging tables in the Redshift Cluster. Check `load_staging_tables` 
in the file `etl.py`
- Transform and load data into fact and dimension tables from the staging tables. Check `insert_tables` 
in the file `etl.py`.